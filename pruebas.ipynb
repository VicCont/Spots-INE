{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'titulo': 'RV00465-24', 'cautelar': False, 'folio': 'https://portal-pautas.ine.mx/pautas5/materiales/MP4/HD/RV00465-24.mp4', 'ruta': True, 'es_publico': True, 'archivoTraduccion': 'Sin archivo', 'directorio': 'tv/PAN', 'partido': 'PAN', 'nombre': 'CAM FED SEN LLEGO LA HORA DEL CAMBIO V2', 'transcripcion': ' Hoy tienes la oportunidad única de cambiar todo lo que está mal en el gobierno federal. Queremos un México para todos. Los programas de adultos mayores que iniciaron en el gobierno panista de Fox ahora los llamaremos 60 y más y el Seguro Popular te lo regresaremos. Con Xochitl y una nueva mayoría en el Congreso se acabarán los abrazos a los criminales. Tú y nuestras familias merecen vivir en paz. Vota por las senadoras y los senadores de Acción Nacional. Por un México sin miedo llegó la hora del cambio. Vota PAN.'}\n"
     ]
    }
   ],
   "source": [
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          titulo  cautelar                                              folio   \n",
      "0     RV00465-24     False  https://portal-pautas.ine.mx/pautas5/materiale...  \\\n",
      "1     RV00466-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
      "2     RV00467-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
      "3     RV00468-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
      "4     RV00470-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
      "...          ...       ...                                                ...   \n",
      "1022  RA01907-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
      "1023  RA01962-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
      "1024  RA00316-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
      "1025  RA00739-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
      "1026  RA00164-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
      "\n",
      "      ruta  es_publico archivoTraduccion   directorio partido   \n",
      "0     True        True       Sin archivo       tv/PAN     PAN  \\\n",
      "1     True        True       Sin archivo       tv/PAN     PAN   \n",
      "2     True        True       Sin archivo       tv/PAN     PAN   \n",
      "3     True        True       Sin archivo       tv/PAN     PAN   \n",
      "4     True        True       Sin archivo       tv/PAN     PAN   \n",
      "...    ...         ...               ...          ...     ...   \n",
      "1022  True        True       Sin archivo    radio/INE     INE   \n",
      "1023  True        True       Sin archivo    radio/INE     INE   \n",
      "1024  True        True       Sin archivo  radio/TEPJF   TEPJF   \n",
      "1025  True        True       Sin archivo  radio/TEPJF   TEPJF   \n",
      "1026  True        True       Sin archivo  radio/FISEL   FISEL   \n",
      "\n",
      "                                         nombre   \n",
      "0       CAM FED SEN LLEGO LA HORA DEL CAMBIO V2  \\\n",
      "1       CAM FED SEN LLEGO LA HORA DE AVANZAR V1   \n",
      "2      CAM FED SEN QRO AVANZAR AGUSTIN DORANTES   \n",
      "3         CAM FED SEN MICH CAMBIO BETO LUCATERO   \n",
      "4          CAM FED SEN SIN CAMBIO EDUARDO ORTIZ   \n",
      "...                                         ...   \n",
      "1022              C1 SC LLAMADO AL VOTO 0424 RD   \n",
      "1023  C1 SC LLAMADO AL VOTO, JUVENTUDES 0424 RD   \n",
      "1024                  ACCIONES AFIRMATIVAS 2024   \n",
      "1025                             MUJERES RADIO    \n",
      "1026                   MATERIAL DE PRUEBA FISEL   \n",
      "\n",
      "                                          transcripcion  \n",
      "0      Hoy tienes la oportunidad única de cambiar to...  \n",
      "1      Hoy tienes la oportunidad única de cambiar to...  \n",
      "2      Ya basta de vivir con miedo. Con una nueva ma...  \n",
      "3      Ya basta de vivir con miedo. Con una nueva ma...  \n",
      "4      Ya basta de vivir con miedo. Con una nueva ma...  \n",
      "...                                                 ...  \n",
      "1022   Este 2 de junio podemos decidir qué queremos ...  \n",
      "1023   Para tener el país que queremos, todas y todo...  \n",
      "1024   Soy afro-mexicano y el Tribunal Electoral gar...  \n",
      "1025   ¿Sabes que hace 70 años las mujeres no podíam...  \n",
      "1026   Comprar tu voto a cambio de dinero o despensa...  \n",
      "\n",
      "[1027 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "keys = data[0].keys()\n",
    "if all(element.keys() == keys for element in data):\n",
    "    # Crea un DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"No todos los elementos tienen las mismas llaves.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>cautelar</th>\n",
       "      <th>folio</th>\n",
       "      <th>ruta</th>\n",
       "      <th>es_publico</th>\n",
       "      <th>archivoTraduccion</th>\n",
       "      <th>directorio</th>\n",
       "      <th>partido</th>\n",
       "      <th>nombre</th>\n",
       "      <th>transcripcion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RV00465-24</td>\n",
       "      <td>False</td>\n",
       "      <td>https://portal-pautas.ine.mx/pautas5/materiale...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Sin archivo</td>\n",
       "      <td>tv/PAN</td>\n",
       "      <td>PAN</td>\n",
       "      <td>CAM FED SEN LLEGO LA HORA DEL CAMBIO V2</td>\n",
       "      <td>Hoy tienes la oportunidad única de cambiar to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RV00466-24</td>\n",
       "      <td>False</td>\n",
       "      <td>https://portal-pautas.ine.mx/pautas5/materiale...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Sin archivo</td>\n",
       "      <td>tv/PAN</td>\n",
       "      <td>PAN</td>\n",
       "      <td>CAM FED SEN LLEGO LA HORA DE AVANZAR V1</td>\n",
       "      <td>Hoy tienes la oportunidad única de cambiar to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RV00467-24</td>\n",
       "      <td>False</td>\n",
       "      <td>https://portal-pautas.ine.mx/pautas5/materiale...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Sin archivo</td>\n",
       "      <td>tv/PAN</td>\n",
       "      <td>PAN</td>\n",
       "      <td>CAM FED SEN QRO AVANZAR AGUSTIN DORANTES</td>\n",
       "      <td>Ya basta de vivir con miedo. Con una nueva ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RV00468-24</td>\n",
       "      <td>False</td>\n",
       "      <td>https://portal-pautas.ine.mx/pautas5/materiale...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Sin archivo</td>\n",
       "      <td>tv/PAN</td>\n",
       "      <td>PAN</td>\n",
       "      <td>CAM FED SEN MICH CAMBIO BETO LUCATERO</td>\n",
       "      <td>Ya basta de vivir con miedo. Con una nueva ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RV00470-24</td>\n",
       "      <td>False</td>\n",
       "      <td>https://portal-pautas.ine.mx/pautas5/materiale...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Sin archivo</td>\n",
       "      <td>tv/PAN</td>\n",
       "      <td>PAN</td>\n",
       "      <td>CAM FED SEN SIN CAMBIO EDUARDO ORTIZ</td>\n",
       "      <td>Ya basta de vivir con miedo. Con una nueva ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       titulo  cautelar                                              folio   \n",
       "0  RV00465-24     False  https://portal-pautas.ine.mx/pautas5/materiale...  \\\n",
       "1  RV00466-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
       "2  RV00467-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
       "3  RV00468-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
       "4  RV00470-24     False  https://portal-pautas.ine.mx/pautas5/materiale...   \n",
       "\n",
       "   ruta  es_publico archivoTraduccion directorio partido   \n",
       "0  True        True       Sin archivo     tv/PAN     PAN  \\\n",
       "1  True        True       Sin archivo     tv/PAN     PAN   \n",
       "2  True        True       Sin archivo     tv/PAN     PAN   \n",
       "3  True        True       Sin archivo     tv/PAN     PAN   \n",
       "4  True        True       Sin archivo     tv/PAN     PAN   \n",
       "\n",
       "                                     nombre   \n",
       "0   CAM FED SEN LLEGO LA HORA DEL CAMBIO V2  \\\n",
       "1   CAM FED SEN LLEGO LA HORA DE AVANZAR V1   \n",
       "2  CAM FED SEN QRO AVANZAR AGUSTIN DORANTES   \n",
       "3     CAM FED SEN MICH CAMBIO BETO LUCATERO   \n",
       "4      CAM FED SEN SIN CAMBIO EDUARDO ORTIZ   \n",
       "\n",
       "                                       transcripcion  \n",
       "0   Hoy tienes la oportunidad única de cambiar to...  \n",
       "1   Hoy tienes la oportunidad única de cambiar to...  \n",
       "2   Ya basta de vivir con miedo. Con una nueva ma...  \n",
       "3   Ya basta de vivir con miedo. Con una nueva ma...  \n",
       "4   Ya basta de vivir con miedo. Con una nueva ma...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # Path to the folder containing the Excel files\n",
    "# folder_path = 'Transmisiones'\n",
    "\n",
    "# # Function to extract period from a given string\n",
    "# def extract_period(period_str):\n",
    "#     match = re.search(r'(\\d{2}/\\d{2}/\\d{4}) al (\\d{2}/\\d{2}/\\d{4})', period_str)\n",
    "#     if match:\n",
    "#         start_date = match.group(1).replace('/', '_')\n",
    "#         end_date = match.group(2).replace('/', '_')\n",
    "#         return start_date, end_date\n",
    "#     return None, None\n",
    "\n",
    "# # List to keep track of newly created files\n",
    "# new_files = []\n",
    "\n",
    "# # Loop through all the files in the folder\n",
    "# for file_name in os.listdir(folder_path):\n",
    "#     if file_name.endswith('.xlsx'):\n",
    "#         file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "#         # Load the Excel file\n",
    "#         xls = pd.ExcelFile(file_path)\n",
    "        \n",
    "#         # Process the \"Excedentes\" sheet\n",
    "#         if 'Excedentes' in xls.sheet_names:\n",
    "#             df_excedentes = pd.read_excel(xls, 'Excedentes')\n",
    "#             period_str = df_excedentes.iloc[5, 0]  # A7 is the 7th row (index 6)\n",
    "#             start_date, end_date = extract_period(period_str)\n",
    "#             if start_date and end_date:\n",
    "#                 new_file_name = f'Excedentes_{start_date}_{end_date}.xlsx'\n",
    "#                 new_file_path = os.path.join(folder_path, new_file_name)\n",
    "#                 df_excedentes.to_excel(new_file_path, index=False)\n",
    "#                 new_files.append(new_file_name)\n",
    "        \n",
    "#         # Process the \"Requerimientos por excedentes\" sheet\n",
    "#         if 'Requerimientos por excedentes' in xls.sheet_names:\n",
    "#             df_requerimientos = pd.read_excel(xls, 'Requerimientos por excedentes')\n",
    "#             period_str = df_requerimientos.iloc[7, 0]  # A9 is the 9th row (index 8)\n",
    "#             start_date, end_date = extract_period(period_str)\n",
    "#             if start_date and end_date:\n",
    "#                 new_file_name = f'Requerimientos_{start_date}_{end_date}.xlsx'\n",
    "#                 new_file_path = os.path.join(folder_path, new_file_name)\n",
    "#                 df_requerimientos.to_excel(new_file_path, index=False)\n",
    "#                 new_files.append(new_file_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the Excel files\n",
    "folder_path = 'Transmisiones'\n",
    "\n",
    "# Function to delete the first N rows of an Excel file\n",
    "def delete_first_n_rows(file_path, n):\n",
    "    df = pd.read_excel(file_path, header=None)  # Read without header\n",
    "    df = df.iloc[n:]  # Drop the first n rows\n",
    "    df.columns = df.iloc[0]  # Set the first row as header\n",
    "    df = df[1:]  # Drop the header row\n",
    "    df.to_excel(file_path, index=False)\n",
    "\n",
    "# Loop through all the files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        if file_name.startswith('Requerimientos_'):\n",
    "            delete_first_n_rows(file_path, 11)\n",
    "        \n",
    "        elif file_name.startswith('Excedentes_'):\n",
    "            delete_first_n_rows(file_path, 8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the Excel files\n",
    "folder_path = 'Transmisiones'\n",
    "\n",
    "# Function to convert an Excel file to CSV\n",
    "def convert_to_csv(file_path, csv_path):\n",
    "    df = pd.read_excel(file_path)  # Read the Excel file\n",
    "    df.to_csv(csv_path, index=False)  # Convert to CSV\n",
    "\n",
    "# Loop through all the files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Define the path for the CSV file\n",
    "        csv_file_name = file_name.replace('.xlsx', '.csv')\n",
    "        csv_path = os.path.join(folder_path, csv_file_name)\n",
    "        \n",
    "        # Convert the Excel file to CSV\n",
    "        convert_to_csv(file_path, csv_path)\n",
    "\n",
    "print(\"Conversion completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos combinados creados exitosamente en formato CSV, ordenados por fecha y hora.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the Excel files\n",
    "folder_path = 'Transmisiones'\n",
    "\n",
    "# Lists to store DataFrames\n",
    "excedentes_list = []\n",
    "requerimientos_list = []\n",
    "\n",
    "# Loop through all the files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        if file_name.startswith('Excedentes_'):\n",
    "            df = pd.read_excel(file_path)\n",
    "            excedentes_list.append(df)\n",
    "        \n",
    "        elif file_name.startswith('Requerimientos_'):\n",
    "            df = pd.read_excel(file_path)\n",
    "            requerimientos_list.append(df)\n",
    "\n",
    "# Concatenate and sort DataFrames in each list\n",
    "if excedentes_list:\n",
    "    excedentes_combined = pd.concat(excedentes_list, ignore_index=True)\n",
    "    excedentes_combined['FECHA TRANSMISIÓN'] = pd.to_datetime(excedentes_combined['FECHA TRANSMISIÓN'], format='%d/%m/%Y')\n",
    "    excedentes_combined['HORA TRANSMISIÓN'] = pd.to_datetime(excedentes_combined['HORA TRANSMISIÓN'], format='%H:%M:%S').dt.time\n",
    "    excedentes_combined = excedentes_combined.sort_values(by=['FECHA TRANSMISIÓN', 'HORA TRANSMISIÓN'])\n",
    "    excedentes_combined.to_csv(os.path.join(folder_path, 'Excedentes_combined.csv'), index=False)\n",
    "\n",
    "if requerimientos_list:\n",
    "    requerimientos_combined = pd.concat(requerimientos_list, ignore_index=True)\n",
    "    requerimientos_combined['Fecha de transmisión del material'] = pd.to_datetime(requerimientos_combined['Fecha de transmisión del material'], dayfirst=True)\n",
    "    requerimientos_combined = requerimientos_combined.sort_values(by=['Fecha de transmisión del material', 'Hora transmisión'])\n",
    "    requerimientos_combined.to_csv(os.path.join(folder_path, 'Requerimientos_combined.csv'), index=False)\n",
    "\n",
    "print('Archivos combinados creados exitosamente en formato CSV, ordenados por fecha y hora.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo Excedentes_combined.csv tiene 78842 renglones.\n",
      "El archivo Requerimientos_combined.csv tiene 45569 renglones.\n",
      "El archivo Excedentes_combined.csv tiene 0 registros repetidos dentro del archivo.\n",
      "El archivo Requerimientos_combined.csv tiene 0 registros repetidos dentro del archivo.\n",
      "Entre los archivos Excedentes_combined.csv y Requerimientos_combined.csv hay 38452 registros repetidos basados en las columnas especificadas.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the combined CSV files\n",
    "folder_path = 'Transmisiones'\n",
    "\n",
    "# Function to count the number of rows in a CSV file\n",
    "def count_rows(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return len(df)\n",
    "\n",
    "# Function to count duplicate rows in a DataFrame\n",
    "def count_duplicates(df):\n",
    "    return df.duplicated().sum()\n",
    "\n",
    "# Function to read a CSV file into a DataFrame\n",
    "def read_csv_file(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Function to count duplicates based on specific columns between two DataFrames\n",
    "def count_cross_duplicates(df1, df2, df1_columns, df2_columns):\n",
    "    df1_subset = df1[df1_columns].copy()\n",
    "    df2_subset = df2[df2_columns].copy()\n",
    "\n",
    "    # Ensure the columns are of the same type (string)\n",
    "    for col in df1_columns:\n",
    "        df1_subset[col] = df1_subset[col].astype(str)\n",
    "    for col in df2_columns:\n",
    "        df2_subset[col] = df2_subset[col].astype(str)\n",
    "\n",
    "    merged_df = pd.merge(df1_subset, df2_subset, left_on=df1_columns, right_on=df2_columns)\n",
    "    return len(merged_df)\n",
    "\n",
    "# File names of the combined CSV files\n",
    "excedentes_combined_file = 'Excedentes_combined.csv'\n",
    "requerimientos_combined_file = 'Requerimientos_combined.csv'\n",
    "\n",
    "# Full paths to the combined CSV files\n",
    "excedentes_combined_path = os.path.join(folder_path, excedentes_combined_file)\n",
    "requerimientos_combined_path = os.path.join(folder_path, requerimientos_combined_file)\n",
    "\n",
    "# Check if the combined files exist and count the rows\n",
    "if os.path.exists(excedentes_combined_path):\n",
    "    excedentes_row_count = count_rows(excedentes_combined_path)\n",
    "    print(f'El archivo {excedentes_combined_file} tiene {excedentes_row_count} renglones.')\n",
    "\n",
    "if os.path.exists(requerimientos_combined_path):\n",
    "    requerimientos_row_count = count_rows(requerimientos_combined_path)\n",
    "    print(f'El archivo {requerimientos_combined_file} tiene {requerimientos_row_count} renglones.')\n",
    "\n",
    "# Read the files and count duplicates within each file\n",
    "if os.path.exists(excedentes_combined_path):\n",
    "    df_excedentes = read_csv_file(excedentes_combined_path)\n",
    "    excedentes_duplicates = count_duplicates(df_excedentes)\n",
    "    print(f'El archivo {excedentes_combined_file} tiene {excedentes_duplicates} registros repetidos dentro del archivo.')\n",
    "\n",
    "if os.path.exists(requerimientos_combined_path):\n",
    "    df_requerimientos = read_csv_file(requerimientos_combined_path)\n",
    "    requerimientos_duplicates = count_duplicates(df_requerimientos)\n",
    "    print(f'El archivo {requerimientos_combined_file} tiene {requerimientos_duplicates} registros repetidos dentro del archivo.')\n",
    "\n",
    "# Check for duplicates between the two files based on specific columns\n",
    "if os.path.exists(excedentes_combined_path) and os.path.exists(requerimientos_combined_path):\n",
    "    cross_duplicates = count_cross_duplicates(\n",
    "        df_excedentes,\n",
    "        df_requerimientos,\n",
    "        df1_columns=['ESTADO', 'FECHA TRANSMISIÓN', 'HORA TRANSMISIÓN'],\n",
    "        df2_columns=['Entidad', 'Fecha de transmisión del material', 'Hora transmisión']\n",
    "    )\n",
    "    print(f'Entre los archivos {excedentes_combined_file} y {requerimientos_combined_file} hay {cross_duplicates} registros repetidos basados en las columnas especificadas.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo Excedentes_combined.csv tiene 78842 renglones.\n",
    "El archivo Requerimientos_combined.csv tiene 45569 renglones.\n",
    "El archivo Excedentes_combined.csv tiene 0 registros repetidos dentro del archivo.\n",
    "El archivo Requerimientos_combined.csv tiene 0 registros repetidos dentro del archivo.\n",
    "Entre los archivos Excedentes_combined.csv y Requerimientos_combined.csv hay 38452 registros repetidos basados en las columnas especificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han eliminado los registros duplicados del archivo Requerimientos_combined.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the combined CSV files\n",
    "folder_path = 'Transmisiones'\n",
    "\n",
    "# Function to read a CSV file into a DataFrame\n",
    "def read_csv_file(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Function to find and remove cross-duplicates from df2 based on df1\n",
    "def remove_cross_duplicates(df1, df2, df1_columns, df2_columns):\n",
    "    df1_subset = df1[df1_columns].copy()\n",
    "    df2_subset = df2[df2_columns].copy()\n",
    "\n",
    "    # Ensure the columns are of the same type (string)\n",
    "    for col in df1_columns:\n",
    "        df1_subset[col] = df1_subset[col].astype(str)\n",
    "    for col in df2_columns:\n",
    "        df2_subset[col] = df2_subset[col].astype(str)\n",
    "\n",
    "    merged_df = pd.merge(df2, df1_subset, how='left', left_on=df2_columns, right_on=df1_columns, indicator=True)\n",
    "    df2_cleaned = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    return df2_cleaned\n",
    "\n",
    "# File names of the combined CSV files\n",
    "excedentes_combined_file = 'Excedentes_combined.csv'\n",
    "requerimientos_combined_file = 'Requerimientos_combined.csv'\n",
    "\n",
    "# Full paths to the combined CSV files\n",
    "excedentes_combined_path = os.path.join(folder_path, excedentes_combined_file)\n",
    "requerimientos_combined_path = os.path.join(folder_path, requerimientos_combined_file)\n",
    "\n",
    "# Check if the combined files exist and read them\n",
    "if os.path.exists(excedentes_combined_path) and os.path.exists(requerimientos_combined_path):\n",
    "    df_excedentes = read_csv_file(excedentes_combined_path)\n",
    "    df_requerimientos = read_csv_file(requerimientos_combined_path)\n",
    "\n",
    "    # Remove duplicates from requerimientos based on excedentes\n",
    "    df_requerimientos_cleaned = remove_cross_duplicates(\n",
    "        df_excedentes,\n",
    "        df_requerimientos,\n",
    "        df1_columns=['ESTADO', 'FECHA TRANSMISIÓN', 'HORA TRANSMISIÓN'],\n",
    "        df2_columns=['Entidad', 'Fecha de transmisión del material', 'Hora transmisión']\n",
    "    )\n",
    "\n",
    "    # Save the cleaned DataFrame to a new CSV file or overwrite the existing one\n",
    "    df_requerimientos_cleaned.to_csv(requerimientos_combined_path, index=False)\n",
    "    print(f'Se han eliminado los registros duplicados del archivo {requerimientos_combined_file}.')\n",
    "else:\n",
    "    print(f'No se encontraron uno o ambos archivos: {excedentes_combined_file}, {requerimientos_combined_file}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\52552\\AppData\\Local\\Temp\\ipykernel_11896\\4079119933.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors='coerce').dt.strftime('%H:%M:%S')\n",
      "C:\\Users\\52552\\AppData\\Local\\Temp\\ipykernel_11896\\4079119933.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors='coerce').dt.strftime('%H:%M:%S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han combinado y guardado los archivos en Transmisiones\\Combined_Spots.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the combined CSV files\n",
    "folder_path = 'Transmisiones'\n",
    "\n",
    "# Function to read a CSV file into a DataFrame\n",
    "def read_csv_file(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Function to rename columns in a DataFrame\n",
    "def rename_columns(df, columns_mapping):\n",
    "    return df.rename(columns=columns_mapping)\n",
    "\n",
    "# Function to standardize date and time formats\n",
    "def standardize_datetime(df, date_col, time_col):\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce').dt.strftime('%H:%M:%S')\n",
    "    return df\n",
    "\n",
    "# File names of the combined CSV files\n",
    "excedentes_combined_file = 'Excedentes_combined.csv'\n",
    "requerimientos_combined_file = 'Requerimientos_combined.csv'\n",
    "\n",
    "# Full paths to the combined CSV files\n",
    "excedentes_combined_path = os.path.join(folder_path, excedentes_combined_file)\n",
    "requerimientos_combined_path = os.path.join(folder_path, requerimientos_combined_file)\n",
    "\n",
    "# Column mapping for renaming\n",
    "columns_mapping = {\n",
    "    'Entidad': 'ESTADO',\n",
    "    'Emisora': 'EMISORA',\n",
    "    'Versión': 'VERSIÓN',\n",
    "    'Folio': 'MATERIAL',\n",
    "    'Actor': 'ACTOR',\n",
    "    'Fecha de transmisión del material': 'FECHA TRANSMISIÓN',\n",
    "    'Hora transmisión': 'HORA TRANSMISIÓN'\n",
    "}\n",
    "\n",
    "# Check if the combined files exist and read them\n",
    "if os.path.exists(excedentes_combined_path) and os.path.exists(requerimientos_combined_path):\n",
    "    df_excedentes = read_csv_file(excedentes_combined_path)\n",
    "    df_requerimientos = read_csv_file(requerimientos_combined_path)\n",
    "\n",
    "    # Rename columns in df_requerimientos\n",
    "    df_requerimientos_renamed = rename_columns(df_requerimientos, columns_mapping)\n",
    "\n",
    "    # Remove duplicate columns before any further processing\n",
    "    df_excedentes = df_excedentes.loc[:, ~df_excedentes.columns.duplicated()]\n",
    "    df_requerimientos_renamed = df_requerimientos_renamed.loc[:, ~df_requerimientos_renamed.columns.duplicated()]\n",
    "\n",
    "    # Standardize date and time formats\n",
    "    df_excedentes = standardize_datetime(df_excedentes, 'FECHA TRANSMISIÓN', 'HORA TRANSMISIÓN')\n",
    "    df_requerimientos_renamed = standardize_datetime(df_requerimientos_renamed, 'FECHA TRANSMISIÓN', 'HORA TRANSMISIÓN')\n",
    "\n",
    "    # Reset index for both DataFrames to avoid InvalidIndexError\n",
    "    df_excedentes.reset_index(drop=True, inplace=True)\n",
    "    df_requerimientos_renamed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Combine the DataFrames with outer join to include all columns\n",
    "    combined_df = pd.concat([df_excedentes, df_requerimientos_renamed], ignore_index=True, sort=False, join='outer')\n",
    "\n",
    "    # Save the combined DataFrame to a new CSV file\n",
    "    combined_file_path = os.path.join(folder_path, 'Transmisiones.csv')\n",
    "    combined_df.to_csv(combined_file_path, index=False)\n",
    "    print(f'Se han combinado y guardado los archivos en {combined_file_path}.')\n",
    "else:\n",
    "    print(f'No se encontraron uno o ambos archivos: {excedentes_combined_file}, {requerimientos_combined_file}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15b0b646c21b8f80a20ac3ca0b6c440335a0ab8e0aad88dec1cfe0fc1eea5a42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
